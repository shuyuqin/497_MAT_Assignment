{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#%tensorflow_version 1.x\n",
    "import tensorflow \n",
    "import numpy as np\n",
    " \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (Reshape,Activation,Attention,MaxPool1D,Dense, Conv1D, Convolution2D, GRU, LSTM, Lambda, Bidirectional, TimeDistributed,\n",
    "                          Dropout, Flatten, LayerNormalization,RepeatVector, Reshape, MaxPooling1D, UpSampling1D, BatchNormalization)\n",
    "import tensorflow.keras.layers as layers\n",
    "import string\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "#sys.path.append('/content/pyNSID/')\n",
    "#import pyNSID as nsid\n",
    "import matplotlib.pyplot as plt\n",
    "#import sidpy as sid\n",
    "#import h5py\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.cluster import KMeans\n",
    "from os.path import join as pjoin\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def make_folder(folder, **kwargs):\n",
    "    \"\"\"\n",
    "    Function that makes new folders\n",
    "    Parameters\n",
    "    ----------'\n",
    "    folder : string\n",
    "        folder where to save\n",
    "    Returns\n",
    "    -------\n",
    "    folder : string\n",
    "        folder where to save\n",
    "    \"\"\"\n",
    "\n",
    "    # Makes folder\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    return (folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pie_data = np.load('pizoresponse.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_data = np.load('resonance.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = np.load('voltage.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_data = np.atleast_3d(res_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = np.concatenate((pie_data,res_data),axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "  \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "  def call(self, inputs):\n",
    "    z_mean, z_log_var = inputs\n",
    "    batch = tf.shape(z_mean)[0]\n",
    "    dim = tf.shape(z_mean)[1]\n",
    "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "class model_builder:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_data, \n",
    "                drop_frac = 0.2, \n",
    "                 layer_size = 128, \n",
    "                 num_ident_blocks = 3, \n",
    "                 l1_norm = 0,\n",
    "                 l1_norm_embedding = 1e-3, \n",
    "                 layer_steps = 2,\n",
    "                 embedding = 16, \n",
    "                 VAE = True,\n",
    "                 coef = 1):\n",
    "    # Sets self.mean and self.std to use in the loss function;\n",
    "#       self.mean = 0\n",
    "#       self.std = 0\n",
    "    \n",
    "      # Sets the L1 norm on the decoder/encoder layers\n",
    "      self.l1_norm = l1_norm\n",
    " \n",
    "      # Sets the fraction of dropout\n",
    "      self.drop_frac = drop_frac\n",
    "      \n",
    "      # saves the shape of the input data\n",
    "      self.data_shape = input_data.shape\n",
    " \n",
    "      # Sets the number of neurons in the encoder/decoder layers\n",
    "      self.layer_size = layer_size\n",
    " \n",
    "      # Sets the number of neurons in the embedding layer\n",
    "      self.embedding = embedding \n",
    " \n",
    "      # Bool to set if the model is a VAE \n",
    "      self.VAE = VAE\n",
    " \n",
    "      # Set the magnitude of the l1 regularization on the embedding layer.\n",
    "      self.l1_norm_embedding = l1_norm_embedding\n",
    " \n",
    "      # sets the number of layers between the residual layer\n",
    "      self.layer_steps = layer_steps\n",
    "\n",
    "      self.coef = coef\n",
    " \n",
    "      # set the number of identity block\n",
    "      self.num_ident_blocks = num_ident_blocks\n",
    " \n",
    "      self.model_constructor(input_data)\n",
    " \n",
    "    def identity_block(self, X, name, \n",
    "                      block):\n",
    "      \n",
    "      # sets the name of the conv layers\n",
    "      LSTM_name_base =  name + '_LSTM_Res_' + block\n",
    "      bn_name_base = name + '_layer_norm_' + block\n",
    " \n",
    " \n",
    "      # output for the residual layer\n",
    "      X_shortcut = X\n",
    " \n",
    "      for i in range(self.layer_steps):\n",
    "            # bidirectional LSTM\n",
    "            X = layers.Bidirectional(LSTM(self.layer_size, \n",
    "                                          return_sequences=True, \n",
    "                                          dropout=self.drop_frac,\n",
    "                                          activity_regularizer=l1(self.l1_norm)), \n",
    "                                    input_shape=(self.data_shape[1]*2, 1))(X)\n",
    "\n",
    "            # TODO, We could add layer norm\n",
    "            X = layers.Activation('relu')(X)  \n",
    "\n",
    "      X = layers.add([X, X_shortcut])\n",
    "  #    X = layers.LayerNormalization(axis = 1, name = bn_name_base + '_res_end')(X)\n",
    "      X = layers.Activation('relu')(X)\n",
    " \n",
    "      return X\n",
    " \n",
    "    def model_constructor(self, input_data):\n",
    "      # defines the input\n",
    "      encoder_input = layers.Input(shape=(self.data_shape[1:]))\n",
    "      X = layers.Flatten()(encoder_input)\n",
    "      X = layers.RepeatVector(1)(X)\n",
    "      X = layers.Permute((2,1))(X)\n",
    "\n",
    "#      X = encoder_input\n",
    "      \n",
    "      for i in range(self.num_ident_blocks):\n",
    "        X = self.identity_block(X, 'encoder', string.ascii_uppercase[i+1])\n",
    "        \n",
    "      # This is in preparation for the embedding layer\n",
    "      X = layers.Bidirectional(LSTM(self.layer_size, \n",
    "                                    return_sequences=False, \n",
    "                                    dropout=self.drop_frac,\n",
    "                                    activity_regularizer=l1(self.l1_norm)), \n",
    "                                    input_shape=(self.data_shape[1]*2, \n",
    "                                                 1))(X)   \n",
    "      \n",
    " #     X = layers.BatchNormalization(axis=1, name='last_encode')(X)\n",
    "      X = layers.Activation('relu')(X)\n",
    "\n",
    "      if self.VAE:\n",
    "            X = layers.Dense(self.embedding, name=\"embedding_pre\")(X)\n",
    "            X = layers.Activation('relu')(X)\n",
    "            X = layers.ActivityRegularization(l1=self.l1_norm_embedding*10**(self.coef))(X)\n",
    "            z_mean = layers.Dense(self.embedding, name=\"z_mean\")(X)\n",
    "            z_log_var = layers.Dense(self.embedding, name=\"z_log_var\")(X)\n",
    "            self.sampling = Sampling()((z_mean, z_log_var))\n",
    "            # update the self.mean and self.std:\n",
    "#            self.mean = z_mean\n",
    "#            self.std = z_log_var\n",
    "\n",
    "      self.encoder_model = Model(inputs=encoder_input, outputs=self.sampling, name='LSTM_encoder')\n",
    "\n",
    "      decoder_input = layers.Input(shape=(self.embedding,), name=\"z_sampling\")\n",
    "\n",
    "      z = layers.Dense(self.embedding, name=\"embedding\")(decoder_input)\n",
    "      z = layers.Activation('relu')(z)\n",
    "      z = layers.ActivityRegularization(l1=self.l1_norm_embedding*10**(self.coef))(z)\n",
    "\n",
    "      X = layers.RepeatVector(self.data_shape[1])(z)\n",
    " \n",
    "      X = layers.Bidirectional(LSTM(self.layer_size, return_sequences=True, \n",
    "                                    dropout=self.drop_frac,\n",
    "                                    activity_regularizer=l1(self.l1_norm)))(X)\n",
    " \n",
    "      # X = layers.BatchNormalization(axis = 1, name = 'fires_decode')(X)\n",
    "      X = layers.Activation('relu')(X)\n",
    " \n",
    "      for i in range(self.num_ident_blocks):\n",
    "        X = self.identity_block(X, 'decoder', string.ascii_uppercase[i+1])\n",
    "    \n",
    " \n",
    " #     X = layers.LayerNormalization(axis=1, name='batch_normal')(X)\n",
    "      X = layers.TimeDistributed(Dense(2, activation='linear'))(X)\n",
    "\n",
    "      self.decoder_model = Model(inputs=decoder_input, outputs=X, name='LSTM_encoder')\n",
    "\n",
    "      outputs = self.decoder_model(self.sampling)\n",
    "\n",
    "      self.vae = tf.keras.Model(inputs=encoder_input, outputs=outputs, name=\"vae\")\n",
    "\n",
    "      # Add KL divergence regularization loss.\n",
    "      kl_loss = -0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
    "      self.vae.add_loss(self.coef*kl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=62000 \n",
    "epoch_per_increase = 1000\n",
    "iteration = (epochs//epoch_per_increase) + 1\n",
    "iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(epochs, epoch_per_increase, initial_beta, beta_per_increase):\n",
    "    best_loss = float('inf')\n",
    "    iteration = (epochs//epoch_per_increase) + 1\n",
    "    model = []\n",
    "    #filepath =folder + '/if_appear_means_bug_happens.hdf5'\n",
    "    \n",
    "    for i in range(iteration):\n",
    "        if i == iteration-1:\n",
    "            training_epochs = epochs - epoch_per_increase*(iteration-1)\n",
    "            if training_epochs <= 0:\n",
    "                break\n",
    "        else:\n",
    "            training_epochs = epoch_per_increase\n",
    "            \n",
    "        beta = initial_beta + beta_per_increase*i\n",
    "        print(beta)\n",
    "        del(model)\n",
    "        model = model_builder(np.atleast_3d(new_data),embedding=16, \n",
    "                      VAE=True, l1_norm_embedding=1e-5,coef=beta)\n",
    "        run_id = '_beta_step_siez=0.0025_' + np.str(model.embedding) + '_layer_size_' + np.str(model.layer_size) + '_l1_norm_' + np.str(model.l1_norm) + '_l1_norm_' + np.str(model.l1_norm_embedding) + '_VAE_' + np.str(model.VAE)\n",
    "        folder = make_folder('piezoresponse+resonacnce/'+'beta='+np.str(beta)+ '_'+run_id)\n",
    "#        if i==30:\n",
    "#            filepath = 'piezoresponse+resonacnce_1/beta=0.0725__beta_step_siez=0.0025_16_layer_size_128_l1_norm_0_l1_norm_1e-05_VAE_True/triple_phase_weights2_epochs=29.hdf5'\n",
    "        if i >0:\n",
    "            print(filepath)\n",
    "            model.vae.load_weights(filepath)\n",
    "        else:\n",
    "            training_epochs = 6000\n",
    "          #model.vae.load_weights('/content/drive/My Drive/papers/Faster_Better_Paper/uniform/0.6000000000000001_V10_fast_100_epochs_per_step_dense_L1_phase_shift_Notebook_w_layernorm_VAE (modified_2)16_layer_size_128_l1_norm_0_l1_norm_0.0001_VAE_True/triple_phase_weights_beta_0.6000000000000001_epochs_begin_450+0048-0.08882.hdf5')\n",
    "            \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "        model.vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
    "        \n",
    "#        beta = beta + i*beta_per_increase\n",
    "        # sets the file path\n",
    "        epoch_begin = i*epoch_per_increase\n",
    "        if i>0:\n",
    "            filepath = folder + '/phase_shift_only' + np.str(beta)+'_epochs_begin_6000+'+np.str(epoch_begin)+'+{epoch:04d}'+'-{loss:.5f}.hdf5'\n",
    "        else:\n",
    "            \n",
    "            filepath = folder + '/phase_shift_only' + np.str(beta)+'_epochs_begin_'+np.str(epoch_begin)+'+{epoch:04d}'+'-{loss:.5f}.hdf5'\n",
    "\n",
    "        # callback for saving checkpoints. Checkpoints are only saved when the model improves\n",
    "        checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='loss',\n",
    "                                                      verbose=0, save_best_only=True,\n",
    "                                                      save_weights_only=True, mode='min')\n",
    "\n",
    "#         if i==0:\n",
    "            \n",
    "#             model.vae.compile(optimizer, loss=KL_Loss(0,0,beta))\n",
    "#         else:\n",
    "#             model.vae.compile(optimizer, loss=KL_Loss(model.mean,model.std,beta))\n",
    "        model.vae.fit(np.atleast_3d(new_data),\n",
    "                      np.atleast_3d(new_data),\n",
    "                      batch_size=180, epochs=training_epochs,callbacks=[checkpoint])\n",
    "    \n",
    "#        total_loss = hist.history['loss'][0]\n",
    "    \n",
    "#        best_loss = total_loss\n",
    "        filepath = folder + '/triple_phase_weights2_epochs='+np.str(i)+'.hdf5'\n",
    "        model.vae.save_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "Train(epochs,epoch_per_increase,0,0.0025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python tensorflow_2_1",
   "language": "python",
   "name": "tf-2_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
