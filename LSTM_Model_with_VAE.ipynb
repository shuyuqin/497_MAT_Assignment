{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow \n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "#\n",
    "import tensorflow as tf\n",
    "#import keras.backend.tensorflow_backend as KTF\n",
    "gpu_devices = tensorflow.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "for device in gpu_devices:\n",
    "    tensorflow.config.experimental.set_memory_growth(device,True)\n",
    "    \n",
    "gpus = tensorflow.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tensorflow.config.experimental.set_visible_devices(gpus[1], 'GPU')\n",
    "    logical_gpus = tensorflow.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorflow.config.experimental.set_visible_devices(gpus[1], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (Reshape,Activation,Attention,MaxPool1D,Dense, Conv1D, Convolution2D, GRU, LSTM, Lambda, Bidirectional, TimeDistributed,\n",
    "                          Dropout, Flatten, LayerNormalization,RepeatVector, Reshape, MaxPooling1D, UpSampling1D, BatchNormalization)\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn import linear_model\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "#from keras import layers as layers\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from scipy import special\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "#from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot\n",
    "import pickle\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "from sklearn.manifold import TSNE\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.decomposition import NMF\n",
    "from scipy import io\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "from scipy.io import loadmat\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_identity_block(X, stage, block, size, n_step, drop_frac, l1_norm):\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'enconde' + str(stage) + block + '_branch'\n",
    "\n",
    "    X_shortcut = X\n",
    "    X = layers.Bidirectional(LSTM(size, return_sequences=True, dropout=drop_frac,\n",
    "                                  activity_regularizer=l1(l1_norm)), input_shape=(n_step, 1))(X)\n",
    "\n",
    "    # X = layers.BatchNormalization(axis = 1, name = bn_name_base + '2a')(X)\n",
    "    X = layers.Activation('relu')(X)\n",
    "\n",
    "    X = layers.Bidirectional(LSTM(size, return_sequences=True, dropout=drop_frac,\n",
    "                                  activity_regularizer=l1(l1_norm)), input_shape=(n_step, 1))(X)\n",
    "\n",
    "    # X = layers.BatchNormalization(axis = 1, name = bn_name_base + '2b')(X)\n",
    "    X = layers.Activation('relu')(X)\n",
    "\n",
    "    X = layers.Bidirectional(LSTM(size, return_sequences=True, dropout=drop_frac,\n",
    "                                  activity_regularizer=l1(l1_norm)), input_shape=(n_step, 1))(X)\n",
    "\n",
    "    \n",
    "\n",
    "    X = layers.add([X, X_shortcut])\n",
    "    X = layers.BatchNormalization(axis = 1, name = bn_name_base + '2c')(X)\n",
    "    X = layers.Activation('relu')(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def decode_identity_block(X, stage, block, size, n_step, drop_frac, l1_norm):\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'decode' + str(stage) + block + '_branch'\n",
    "\n",
    "    X_shortcut = X\n",
    "    X = layers.Bidirectional(LSTM(size, return_sequences=True, dropout=drop_frac,\n",
    "                                  activity_regularizer=l1(l1_norm)))(X)\n",
    "    # X = layers.BatchNormalization(axis = 1, name = bn_name_base + '2a')(X)\n",
    "    X = layers.Activation('relu')(X)\n",
    "\n",
    "    X = layers.Bidirectional(LSTM(size, return_sequences=True, dropout=drop_frac,\n",
    "                                  activity_regularizer=l1(l1_norm)))(X)\n",
    "\n",
    "    # X = layers.BatchNormalization(axis = 1, name = bn_name_base + '2b')(X)\n",
    "    X = layers.Activation('relu')(X)\n",
    "\n",
    "    X = layers.Bidirectional(LSTM(size, return_sequences=True, dropout=drop_frac,\n",
    "                                  activity_regularizer=l1(l1_norm)))(X)\n",
    "    # \n",
    "\n",
    "    X = layers.add([X, X_shortcut])\n",
    "    X = layers.BatchNormalization(axis = 1, name = bn_name_base + '2c')(X)\n",
    "    X = layers.Activation('relu')(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "    \n",
    "\n",
    "def ResNet50(lr=3e-5, size=128, drop_frac=0, n_step=96, embedding=16, l1_norm=1e-5, coef=1e-2):\n",
    "    X_input = layers.Input(shape=(n_step, 1))\n",
    "    X = X_input\n",
    "\n",
    "    X = encode_identity_block(X, 2, 'b', size, n_step, drop_frac, l1_norm)\n",
    "    X = encode_identity_block(X, 2, 'c', size, n_step, drop_frac, l1_norm)\n",
    "    X = encode_identity_block(X, 2, 'd', size, n_step, drop_frac, l1_norm)\n",
    "\n",
    "    X = layers.Bidirectional(LSTM(size, return_sequences=False, dropout=drop_frac,\n",
    "                                  activity_regularizer=l1(l1_norm)), input_shape=(n_step, 1))(X)\n",
    "    #X = layers.BatchNormalization(axis=1, name='last_encode')(X)\n",
    "    X = layers.Activation('relu')(X)\n",
    "#    X = layers.Dense(embedding, activation='relu', name='embedding_layer', activity_regularizer=l1(l1_norm))(X)\n",
    "   \n",
    "    sd = 0.5 * Dense(units = embedding, name='standard_deviation',\n",
    "                     activation = 'relu', activity_regularizer=l1(l1_norm))(X) \n",
    "    \n",
    "    mn = Dense(units = embedding, name='mean_value',activation = 'relu', \n",
    "               activity_regularizer=l1(l1_norm))(X)\n",
    "    \n",
    "    def tensor_stack(layer):\n",
    "        return K.random_normal(K.stack([tensorflow.shape(layer)[0], embedding]), mean=1.0) \n",
    "   \n",
    "    epsilon = Lambda(lambda x: tensor_stack(x))(mn)\n",
    "    \n",
    "    def tensor_multi(layer):\n",
    "        return tensorflow.multiply(epsilon, tensorflow.exp(layer))\n",
    "    \n",
    "    \n",
    "    z = Lambda(lambda x: tensor_multi(x))(sd)\n",
    "    X = mn + z\n",
    "    \n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    X = layers.RepeatVector(n_step)(X)\n",
    "\n",
    "    X = layers.Bidirectional(LSTM(size, return_sequences=True, dropout=drop_frac,\n",
    "                                  activity_regularizer=l1(l1_norm)))(X)\n",
    "\n",
    "    # X = layers.BatchNormalization(axis = 1, name = 'fires_decode')(X)\n",
    "    X = layers.Activation('relu')(X)\n",
    "\n",
    "    X = decode_identity_block(X, 2, 'b', size, n_step, drop_frac, l1_norm)\n",
    "    X = decode_identity_block(X, 2, 'c', size, n_step, drop_frac, l1_norm)\n",
    "    X = decode_identity_block(X, 2, 'd', size, n_step, drop_frac, l1_norm)\n",
    "\n",
    "    X = layers.BatchNormalization(axis=1, name='batch_normal')(X)\n",
    "    X = layers.TimeDistributed(Dense(1, activation='linear'))(X)\n",
    "    model = Model(inputs=X_input, outputs=X, name='ResNet50')\n",
    "    model.compile(Adam(lr), loss=kl_loss_1(sd,mn,coef))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_loss_1(sd, mn, coef):\n",
    "    latent_loss = -coef * tensorflow.reduce_sum(1.0 + 2.0 * sd - tensorflow.square(mn-1)\n",
    "                                               - tensorflow.exp(2.0 * sd), 1)\n",
    "    def loss(y_true, y_pred):\n",
    "        \n",
    "        img_loss = tf.reduce_sum(tf.math.squared_difference(y_pred, y_true), 1)\n",
    "        return tf.reduce_mean(img_loss + latent_loss)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lst = ResNet50()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ResNet50\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 96, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_60 (Bidirectional (None, 96, 256)      133120      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 96, 256)      0           bidirectional_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_61 (Bidirectional (None, 96, 256)      394240      activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 96, 256)      0           bidirectional_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_62 (Bidirectional (None, 96, 256)      394240      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 96, 256)      0           bidirectional_62[0][0]           \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "enconde2b_branch2c (BatchNormal (None, 96, 256)      384         add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 96, 256)      0           enconde2b_branch2c[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_63 (Bidirectional (None, 96, 256)      394240      activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 96, 256)      0           bidirectional_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_64 (Bidirectional (None, 96, 256)      394240      activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 96, 256)      0           bidirectional_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_65 (Bidirectional (None, 96, 256)      394240      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 96, 256)      0           bidirectional_65[0][0]           \n",
      "                                                                 activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "enconde2c_branch2c (BatchNormal (None, 96, 256)      384         add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 96, 256)      0           enconde2c_branch2c[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_66 (Bidirectional (None, 96, 256)      394240      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 96, 256)      0           bidirectional_66[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_67 (Bidirectional (None, 96, 256)      394240      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 96, 256)      0           bidirectional_67[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_68 (Bidirectional (None, 96, 256)      394240      activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 96, 256)      0           bidirectional_68[0][0]           \n",
      "                                                                 activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "enconde2d_branch2c (BatchNormal (None, 96, 256)      384         add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 96, 256)      0           enconde2d_branch2c[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_69 (Bidirectional (None, 256)          394240      activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 256)          0           bidirectional_69[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "standard_deviation (Dense)      (None, 16)           4112        activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_5 (TensorFlowOp [(None, 16)]         0           standard_deviation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "mean_value (Dense)              (None, 16)           4112        activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 16)           0           tf_op_layer_mul_5[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_21 (TensorFlowO [(None, 16)]         0           mean_value[0][0]                 \n",
      "                                                                 lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 16)           0           tf_op_layer_add_21[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 96, 16)       0           activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_70 (Bidirectional (None, 96, 256)      148480      repeat_vector_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 96, 256)      0           bidirectional_70[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_71 (Bidirectional (None, 96, 256)      394240      activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 96, 256)      0           bidirectional_71[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_72 (Bidirectional (None, 96, 256)      394240      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 96, 256)      0           bidirectional_72[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_73 (Bidirectional (None, 96, 256)      394240      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 96, 256)      0           bidirectional_73[0][0]           \n",
      "                                                                 activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decode2b_branch2c (BatchNormali (None, 96, 256)      384         add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 96, 256)      0           decode2b_branch2c[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_74 (Bidirectional (None, 96, 256)      394240      activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 96, 256)      0           bidirectional_74[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_75 (Bidirectional (None, 96, 256)      394240      activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 96, 256)      0           bidirectional_75[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_76 (Bidirectional (None, 96, 256)      394240      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 96, 256)      0           bidirectional_76[0][0]           \n",
      "                                                                 activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decode2c_branch2c (BatchNormali (None, 96, 256)      384         add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 96, 256)      0           decode2c_branch2c[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_77 (Bidirectional (None, 96, 256)      394240      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 96, 256)      0           bidirectional_77[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_78 (Bidirectional (None, 96, 256)      394240      activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 96, 256)      0           bidirectional_78[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_79 (Bidirectional (None, 96, 256)      394240      activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 96, 256)      0           bidirectional_79[0][0]           \n",
      "                                                                 activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decode2d_branch2c (BatchNormali (None, 96, 256)      384         add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 96, 256)      0           decode2d_branch2c[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normal (BatchNormalizatio (None, 96, 256)      384         activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 96, 1)        257         batch_normal[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 7,389,089\n",
      "Trainable params: 7,387,745\n",
      "Non-trainable params: 1,344\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Lst.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pieoresponse = np.load('pizoresponse.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"Piezoresponse_Model/weights-lstm_vae+{epoch:02d}.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath, monitor='loss', \n",
    "                               verbose=0, save_best_only=True, save_weights_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python2.7/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 3600 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[96,128,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node bidirectional_79/forward_lstm_79_1/TensorArrayStack/TensorArrayGatherV3}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[loss/add/_2983]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[96,128,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node bidirectional_79/forward_lstm_79_1/TensorArrayStack/TensorArrayGatherV3}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-5b29ffd8380f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m Lst.fit(x= pieoresponse, y = pieoresponse,epochs=10,\n\u001b[0;32m----> 2\u001b[0;31m               batch_size=128,shuffle = True,callbacks=[checkpointer])\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/dist-packages/tensorflow_core/python/keras/backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m/usr/lib/python2.7/dist-packages/tensorflow_core/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[96,128,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node bidirectional_79/forward_lstm_79_1/TensorArrayStack/TensorArrayGatherV3}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[loss/add/_2983]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[96,128,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node bidirectional_79/forward_lstm_79_1/TensorArrayStack/TensorArrayGatherV3}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ],
   "source": [
    "Lst.fit(x= pieoresponse, y = pieoresponse,epochs=10,\n",
    "              batch_size=128,shuffle = True,callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
