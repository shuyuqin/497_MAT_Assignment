{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (Reshape,Activation,Attention,MaxPool1D,Dense, Conv1D, Lambda,Convolution2D, GRU, LSTM, Bidirectional, TimeDistributed,\n",
    "                          Dropout, Flatten, LayerNormalization,RepeatVector, Reshape, MaxPooling1D, UpSampling1D, BatchNormalization)\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn import linear_model\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "# from keras import layers as layers\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from scipy import special\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "#from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot\n",
    "import pickle\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Identity_Block(X, time_step, kernel_size, drop_out ):\n",
    "    \n",
    "    x = Conv1D(time_step, kernel_size, padding='same')(X)\n",
    "    x = LayerNormalization(axis=1)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = tf.transpose(x, (0, 2, 1)) \n",
    "    x = Attention()([x,x])\n",
    "    x = tf.transpose(x, (0, 2, 1))\n",
    "    x = Dropout(drop_out)(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv1D_Block(X, time_step, kernel_size, drop_out):\n",
    "    X_input = X\n",
    "    x = Conv1D(time_step, kernel_size, padding='same')(X)\n",
    "    x = LayerNormalization(axis=1)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = tf.transpose(x, (0, 2, 1)) \n",
    "    x = Attention()([x,x])\n",
    "    x = tf.transpose(x, (0, 2, 1)) \n",
    "    \n",
    "    x = Conv1D(time_step, kernel_size, padding='same')(x)\n",
    "    x = LayerNormalization(axis=1)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = tf.transpose(x, (0, 2, 1)) \n",
    "    x = Attention()([x,x])\n",
    "    x = tf.transpose(x, (0, 2, 1)) \n",
    "    \n",
    "    x = Conv1D(time_step, kernel_size, padding='same')(x)\n",
    "    x = LayerNormalization(axis=1)(x)\n",
    "    x = tf.transpose(x, (0, 2, 1)) \n",
    "    x = Attention()([x,x])\n",
    "    x = tf.transpose(x, (0, 2, 1))\n",
    "    x = Dropout(drop_out)(x)\n",
    "    \n",
    "    x = layers.add([X_input,x])\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv1D_Pie(size = 12, time_step = 128, kernel_size = 3, lr = 2e-5, embedding = 8, \n",
    "                  n_step = 96, drop_out=0.2, l2_norm = 5e-7):\n",
    "    X_input = layers.Input(shape=(n_step,1))\n",
    "    X = X_input\n",
    "#    Embedding_out = layers.Input(shape=(8,))\n",
    "    \n",
    "    x = Identity_Block(X, time_step, kernel_size, drop_out)\n",
    "    x = Conv1D_Block(x, time_step, kernel_size, drop_out)\n",
    "    x = Conv1D_Block(x, time_step, kernel_size, drop_out)\n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "    x = Identity_Block(x, time_step, kernel_size, drop_out)\n",
    "    x = Conv1D_Block(x, time_step, kernel_size, drop_out)\n",
    "    x = Conv1D_Block(x, time_step, kernel_size, drop_out)\n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "# x = Conv1D(128, 3, activation='relu', padding='same')(x)\n",
    "# encoded = MaxPooling1D(2, padding='same')(x)\n",
    "    x = tf.transpose(x, (0, 2, 1)) \n",
    "    encoded = Bidirectional(LSTM(size, return_sequences=False, dropout=drop_out))(x)\n",
    "# # at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
    "    embedding = Dense(embedding, activation='relu',activity_regularizer=l2(l2_norm))(encoded)\n",
    "    \n",
    "    repeat = RepeatVector(128)(embedding)\n",
    "    x = tf.transpose(repeat, (0, 2, 1)) \n",
    "    \n",
    "    x = Identity_Block(x, time_step, kernel_size, drop_out)\n",
    "    x = Conv1D_Block(x, time_step, kernel_size, drop_out)\n",
    "    x = Conv1D_Block(x, time_step, kernel_size, drop_out)\n",
    "    x = UpSampling1D(2)(x)\n",
    "    x = Identity_Block(x, time_step, kernel_size, drop_out)\n",
    "    x = Conv1D_Block(x, time_step, kernel_size, drop_out)\n",
    "    x = Conv1D_Block(x, time_step, kernel_size, drop_out)\n",
    "    x = UpSampling1D(2)(x)\n",
    "    x = Identity_Block(x, time_step, kernel_size, drop_out)\n",
    "    x = Conv1D_Block(x, time_step, kernel_size, drop_out)\n",
    "    x = Conv1D_Block(x, time_step, kernel_size, drop_out)\n",
    "    x = UpSampling1D(3)(x)\n",
    "    x = Identity_Block(x, time_step, kernel_size, drop_out)\n",
    "    x = Conv1D_Block(x, time_step, kernel_size, drop_out)\n",
    "    x = Conv1D_Block(x, time_step, kernel_size, drop_out)\n",
    "    decoded = Conv1D(1, 3, activation='linear', padding='same')(x)\n",
    "#x = UpSampling1D(2)(x)\n",
    "\n",
    "    model = Model(X_input, decoded,name = 'Convolutional_1D_with_Attention')\n",
    "    model.compile(Adam(lr), loss='mse')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
